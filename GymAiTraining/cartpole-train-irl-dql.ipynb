{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T06:31:12.293718Z",
     "start_time": "2025-01-03T06:31:10.809133Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define the neural network model\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T06:31:12.302853Z",
     "start_time": "2025-01-03T06:31:12.296724Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "def save_model(agent, filename):\n",
    "    # Save the model's state_dict (weights) for both the local and target Q networks\n",
    "    torch.save({\n",
    "        'state_dict_local': agent.qnetwork_local.state_dict(),\n",
    "        'state_dict_target': agent.qnetwork_target.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "        'seed': agent.seed,\n",
    "        'state_size': agent.state_size,\n",
    "        'action_size': agent.action_size\n",
    "    }, filename)\n",
    "    \n",
    "def load_model(filename, state_size, action_size, seed, lr):\n",
    "    # Initialize a new DQN agent\n",
    "    agent = DQNAgent(state_size, action_size, seed, lr)\n",
    "    \n",
    "    # Load the saved model's state_dicts into the agent\n",
    "    checkpoint = torch.load(filename)\n",
    "    agent.qnetwork_local.load_state_dict(checkpoint['state_dict_local'])\n",
    "    agent.qnetwork_target.load_state_dict(checkpoint['state_dict_target'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Optionally, return additional info if needed\n",
    "    agent.seed = checkpoint['seed']\n",
    "    agent.state_size = checkpoint['state_size']\n",
    "    agent.action_size = checkpoint['action_size']\n",
    "    \n",
    "    return agent"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T06:31:12.735429Z",
     "start_time": "2025-01-03T06:31:12.725446Z"
    }
   },
   "source": [
    "# Define the DQN agent class\n",
    "class DQNAgent:\n",
    "    # Initialize the DQN agent\n",
    "    def __init__(self, state_size, action_size, seed, lr):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > 64:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma=0.99)\n",
    "\n",
    "    # Choose an action based on the current state\n",
    "    def act(self, state, eps=0.):\n",
    "        #print(type(state))\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state_tensor)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if np.random.random() > eps:\n",
    "            return action_values.argmax(dim=1).item()\n",
    "        else:\n",
    "            return np.random.randint(self.action_size)\n",
    "\n",
    "    # Learn from batch of experiences\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T06:31:21.875521Z",
     "start_time": "2025-01-03T06:31:13.612339Z"
    }
   },
   "source": [
    "# Initialize the environment and the agent\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "from cartpole_irl import CustomCartPoleEnv\n",
    "import machine\n",
    "\n",
    "gym.register(\n",
    "    id=\"CustomCartPole-v2\",\n",
    "    entry_point=CustomCartPoleEnv,\n",
    ")\n",
    "# Set up the environment\n",
    "arduino, printer = machine.setup()\n",
    "env = gym.make(\"CustomCartPole-v2\", arduino=arduino, printer=printer)\n",
    "\n",
    "# Define training parameters\n",
    "num_episodes = 5000\n",
    "max_steps_per_episode = 1000\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.2\n",
    "epsilon_decay_rate = 0.998\n",
    "gamma = 0.9\n",
    "lr = 0.0005\n",
    "buffer_size = 10000\n",
    "buffer = deque(maxlen=buffer_size)\n",
    "batch_size = 128\n",
    "update_frequency = 20\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "print(f\"Input: {input_dim}, Output: {output_dim}\")\n",
    "new_agent = DQNAgent(input_dim, output_dim, seed=170715, lr = lr)\n",
    "#new_agent = load_model(\"agentirlv3.pth\", input_dim, output_dim, seed=170715, lr=lr)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 4, Output: 7\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T06:31:48.208591Z",
     "start_time": "2025-01-03T06:31:48.202762Z"
    }
   },
   "cell_type": "code",
   "source": "num_episodes = 50000",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-03T17:14:29.648756Z",
     "start_time": "2025-01-03T06:31:54.088696Z"
    }
   },
   "source": [
    "# Training loop\n",
    "#env = gym.make(\"CustomCartPole-v2\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay_rate ** episode))\n",
    "\n",
    "    # Run one episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Choose and perform an action\n",
    "        action = new_agent.act(state, epsilon)\n",
    "        nextStep = env.step(action)\n",
    "        next_state = nextStep[0]\n",
    "        reward = nextStep[1]\n",
    "        done = nextStep[2]\n",
    "        \n",
    "        buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(buffer) >= batch_size:\n",
    "            batch = random.sample(buffer, batch_size)\n",
    "            # Update the agent's knowledge\n",
    "            new_agent.learn(batch, gamma)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # Check if the episode has ended\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if (episode + 1) % update_frequency == 0:\n",
    "        machine.calibrate(printer)\n",
    "        print(f\"Episode {episode + 1}: Finished training\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20: Finished training\n",
      "Episode 40: Finished training\n",
      "Episode 60: Finished training\n",
      "Episode 80: Finished training\n",
      "Episode 100: Finished training\n",
      "Episode 120: Finished training\n",
      "Episode 140: Finished training\n",
      "Episode 160: Finished training\n",
      "Episode 180: Finished training\n",
      "Episode 200: Finished training\n",
      "Episode 220: Finished training\n",
      "Episode 240: Finished training\n",
      "Episode 260: Finished training\n",
      "Episode 280: Finished training\n",
      "Episode 300: Finished training\n",
      "Episode 320: Finished training\n",
      "Episode 340: Finished training\n",
      "Episode 360: Finished training\n",
      "Episode 380: Finished training\n",
      "Episode 400: Finished training\n",
      "Episode 420: Finished training\n",
      "Episode 440: Finished training\n",
      "Episode 460: Finished training\n",
      "Episode 480: Finished training\n",
      "Episode 500: Finished training\n",
      "Episode 520: Finished training\n",
      "Episode 540: Finished training\n",
      "Episode 560: Finished training\n",
      "Episode 580: Finished training\n",
      "Episode 600: Finished training\n",
      "Episode 620: Finished training\n",
      "Episode 640: Finished training\n",
      "Episode 660: Finished training\n",
      "Episode 680: Finished training\n",
      "Episode 700: Finished training\n",
      "Episode 720: Finished training\n",
      "Episode 740: Finished training\n",
      "Episode 760: Finished training\n",
      "Episode 780: Finished training\n",
      "Episode 800: Finished training\n",
      "Episode 820: Finished training\n",
      "Episode 840: Finished training\n",
      "Episode 860: Finished training\n",
      "Episode 880: Finished training\n",
      "Episode 900: Finished training\n",
      "Episode 920: Finished training\n",
      "Episode 940: Finished training\n",
      "Episode 960: Finished training\n",
      "Episode 980: Finished training\n",
      "Episode 1000: Finished training\n",
      "Episode 1020: Finished training\n",
      "Episode 1040: Finished training\n",
      "Episode 1060: Finished training\n",
      "Episode 1080: Finished training\n",
      "Episode 1100: Finished training\n",
      "Episode 1120: Finished training\n",
      "Episode 1140: Finished training\n",
      "Episode 1160: Finished training\n",
      "Episode 1180: Finished training\n",
      "Episode 1200: Finished training\n",
      "Episode 1220: Finished training\n",
      "Episode 1240: Finished training\n",
      "Episode 1260: Finished training\n",
      "Episode 1280: Finished training\n",
      "Episode 1300: Finished training\n",
      "Episode 1320: Finished training\n",
      "Episode 1340: Finished training\n",
      "Episode 1360: Finished training\n",
      "Episode 1380: Finished training\n",
      "Episode 1400: Finished training\n",
      "Episode 1420: Finished training\n",
      "Episode 1440: Finished training\n",
      "Episode 1460: Finished training\n",
      "Episode 1480: Finished training\n",
      "Episode 1500: Finished training\n",
      "Episode 1520: Finished training\n",
      "Episode 1540: Finished training\n",
      "Episode 1560: Finished training\n",
      "Episode 1580: Finished training\n",
      "Episode 1600: Finished training\n",
      "Episode 1620: Finished training\n",
      "Episode 1640: Finished training\n",
      "Episode 1660: Finished training\n",
      "Episode 1680: Finished training\n",
      "Episode 1700: Finished training\n",
      "Episode 1720: Finished training\n",
      "Episode 1740: Finished training\n",
      "Episode 1760: Finished training\n",
      "Episode 1780: Finished training\n",
      "Episode 1800: Finished training\n",
      "Episode 1820: Finished training\n",
      "Episode 1840: Finished training\n",
      "Episode 1860: Finished training\n",
      "Episode 1880: Finished training\n",
      "Episode 1900: Finished training\n",
      "Episode 1920: Finished training\n",
      "Episode 1940: Finished training\n",
      "Episode 1960: Finished training\n",
      "Episode 1980: Finished training\n",
      "Episode 2000: Finished training\n",
      "Episode 2020: Finished training\n",
      "Episode 2040: Finished training\n",
      "Episode 2060: Finished training\n",
      "Episode 2080: Finished training\n",
      "Episode 2100: Finished training\n",
      "Episode 2120: Finished training\n",
      "Episode 2140: Finished training\n",
      "Episode 2160: Finished training\n",
      "Episode 2180: Finished training\n",
      "Episode 2200: Finished training\n",
      "Episode 2220: Finished training\n",
      "Episode 2240: Finished training\n",
      "Episode 2260: Finished training\n",
      "Episode 2280: Finished training\n",
      "Episode 2300: Finished training\n",
      "Episode 2320: Finished training\n",
      "Episode 2340: Finished training\n",
      "Episode 2360: Finished training\n",
      "Episode 2380: Finished training\n",
      "Episode 2400: Finished training\n",
      "Episode 2420: Finished training\n",
      "Episode 2440: Finished training\n",
      "Episode 2460: Finished training\n",
      "Episode 2480: Finished training\n",
      "Episode 2500: Finished training\n",
      "Episode 2520: Finished training\n",
      "Episode 2540: Finished training\n",
      "Episode 2560: Finished training\n",
      "Episode 2580: Finished training\n",
      "Episode 2600: Finished training\n",
      "Episode 2620: Finished training\n",
      "Episode 2640: Finished training\n",
      "Episode 2660: Finished training\n",
      "Episode 2680: Finished training\n",
      "Episode 2700: Finished training\n",
      "Episode 2720: Finished training\n",
      "Episode 2740: Finished training\n",
      "Episode 2760: Finished training\n",
      "Episode 2780: Finished training\n",
      "Episode 2800: Finished training\n",
      "Episode 2820: Finished training\n",
      "Episode 2840: Finished training\n",
      "Episode 2860: Finished training\n",
      "Episode 2880: Finished training\n",
      "Episode 2900: Finished training\n",
      "Episode 2920: Finished training\n",
      "Episode 2940: Finished training\n",
      "Episode 2960: Finished training\n",
      "Episode 2980: Finished training\n",
      "Episode 3000: Finished training\n",
      "Episode 3020: Finished training\n",
      "Episode 3040: Finished training\n",
      "Episode 3060: Finished training\n",
      "Episode 3080: Finished training\n",
      "Episode 3100: Finished training\n",
      "Episode 3120: Finished training\n",
      "Episode 3140: Finished training\n",
      "Episode 3160: Finished training\n",
      "Episode 3180: Finished training\n",
      "Episode 3200: Finished training\n",
      "Episode 3220: Finished training\n",
      "Episode 3240: Finished training\n",
      "Episode 3260: Finished training\n",
      "Episode 3280: Finished training\n",
      "Episode 3300: Finished training\n",
      "Episode 3320: Finished training\n",
      "Episode 3340: Finished training\n",
      "Episode 3360: Finished training\n",
      "Episode 3380: Finished training\n",
      "Episode 3400: Finished training\n",
      "Episode 3420: Finished training\n",
      "Episode 3440: Finished training\n",
      "Episode 3460: Finished training\n",
      "Episode 3480: Finished training\n",
      "Episode 3500: Finished training\n",
      "Episode 3520: Finished training\n",
      "Episode 3540: Finished training\n",
      "Episode 3560: Finished training\n",
      "Episode 3580: Finished training\n",
      "Episode 3600: Finished training\n",
      "Episode 3620: Finished training\n",
      "Episode 3640: Finished training\n",
      "Episode 3660: Finished training\n",
      "Episode 3680: Finished training\n",
      "Episode 3700: Finished training\n",
      "Episode 3720: Finished training\n",
      "Episode 3740: Finished training\n",
      "Episode 3760: Finished training\n",
      "Episode 3780: Finished training\n",
      "Episode 3800: Finished training\n",
      "Episode 3820: Finished training\n",
      "Episode 3840: Finished training\n",
      "Episode 3860: Finished training\n",
      "Episode 3880: Finished training\n",
      "Episode 3900: Finished training\n",
      "Episode 3920: Finished training\n",
      "Episode 3940: Finished training\n",
      "Episode 3960: Finished training\n",
      "Episode 3980: Finished training\n",
      "Episode 4000: Finished training\n",
      "Episode 4020: Finished training\n",
      "Episode 4040: Finished training\n",
      "Episode 4060: Finished training\n",
      "Episode 4080: Finished training\n",
      "Episode 4100: Finished training\n",
      "Episode 4120: Finished training\n",
      "Episode 4140: Finished training\n",
      "Episode 4160: Finished training\n",
      "Episode 4180: Finished training\n",
      "Episode 4200: Finished training\n",
      "Episode 4220: Finished training\n",
      "Episode 4240: Finished training\n",
      "Episode 4260: Finished training\n",
      "Episode 4280: Finished training\n",
      "Episode 4300: Finished training\n",
      "Episode 4320: Finished training\n",
      "Episode 4340: Finished training\n",
      "Episode 4360: Finished training\n",
      "Episode 4380: Finished training\n",
      "Episode 4400: Finished training\n",
      "Episode 4420: Finished training\n",
      "Episode 4440: Finished training\n",
      "Episode 4460: Finished training\n",
      "Episode 4480: Finished training\n",
      "Episode 4500: Finished training\n",
      "Episode 4520: Finished training\n",
      "Episode 4540: Finished training\n",
      "Episode 4560: Finished training\n",
      "Episode 4580: Finished training\n",
      "Episode 4600: Finished training\n",
      "Episode 4620: Finished training\n",
      "Episode 4640: Finished training\n",
      "Episode 4660: Finished training\n",
      "Episode 4680: Finished training\n",
      "Episode 4700: Finished training\n",
      "Episode 4720: Finished training\n",
      "Episode 4740: Finished training\n",
      "Episode 4760: Finished training\n",
      "Episode 4780: Finished training\n",
      "Episode 4800: Finished training\n",
      "Episode 4820: Finished training\n",
      "Episode 4840: Finished training\n",
      "Episode 4860: Finished training\n",
      "Episode 4880: Finished training\n",
      "Episode 4900: Finished training\n",
      "Episode 4920: Finished training\n",
      "Episode 4940: Finished training\n",
      "Episode 4960: Finished training\n",
      "Episode 4980: Finished training\n",
      "Episode 5000: Finished training\n",
      "Episode 5020: Finished training\n",
      "Episode 5040: Finished training\n",
      "Episode 5060: Finished training\n",
      "Episode 5080: Finished training\n",
      "Episode 5100: Finished training\n",
      "Episode 5120: Finished training\n",
      "Episode 5140: Finished training\n",
      "Episode 5160: Finished training\n",
      "Episode 5180: Finished training\n",
      "Episode 5200: Finished training\n",
      "Episode 5220: Finished training\n",
      "Episode 5240: Finished training\n",
      "Episode 5260: Finished training\n",
      "Episode 5280: Finished training\n",
      "Episode 5300: Finished training\n",
      "Episode 5320: Finished training\n",
      "Episode 5340: Finished training\n",
      "Episode 5360: Finished training\n",
      "Episode 5380: Finished training\n",
      "Episode 5400: Finished training\n",
      "Episode 5420: Finished training\n",
      "Episode 5440: Finished training\n",
      "Episode 5460: Finished training\n",
      "Episode 5480: Finished training\n",
      "Episode 5500: Finished training\n",
      "Episode 5520: Finished training\n",
      "Episode 5540: Finished training\n",
      "Episode 5560: Finished training\n",
      "Episode 5580: Finished training\n",
      "Episode 5600: Finished training\n",
      "Episode 5620: Finished training\n",
      "Episode 5640: Finished training\n",
      "Episode 5660: Finished training\n",
      "Episode 5680: Finished training\n",
      "Episode 5700: Finished training\n",
      "Episode 5720: Finished training\n",
      "Episode 5740: Finished training\n",
      "Episode 5760: Finished training\n",
      "Episode 5780: Finished training\n",
      "Episode 5800: Finished training\n",
      "Episode 5820: Finished training\n",
      "Episode 5840: Finished training\n",
      "Episode 5860: Finished training\n",
      "Episode 5880: Finished training\n",
      "Episode 5900: Finished training\n",
      "Episode 5920: Finished training\n",
      "Episode 5940: Finished training\n",
      "Episode 5960: Finished training\n",
      "Episode 5980: Finished training\n",
      "Episode 6000: Finished training\n",
      "Episode 6020: Finished training\n",
      "Episode 6040: Finished training\n",
      "Episode 6060: Finished training\n",
      "Episode 6080: Finished training\n",
      "Episode 6100: Finished training\n",
      "Episode 6120: Finished training\n",
      "Episode 6140: Finished training\n",
      "Episode 6160: Finished training\n",
      "Episode 6180: Finished training\n",
      "Episode 6200: Finished training\n",
      "Episode 6220: Finished training\n",
      "Episode 6240: Finished training\n",
      "Episode 6260: Finished training\n",
      "Episode 6280: Finished training\n",
      "Episode 6300: Finished training\n",
      "Episode 6320: Finished training\n",
      "Episode 6340: Finished training\n",
      "Episode 6360: Finished training\n",
      "Episode 6380: Finished training\n",
      "Episode 6400: Finished training\n",
      "Episode 6420: Finished training\n",
      "Episode 6440: Finished training\n",
      "Episode 6460: Finished training\n",
      "Episode 6480: Finished training\n",
      "Episode 6500: Finished training\n",
      "Episode 6520: Finished training\n",
      "Episode 6540: Finished training\n",
      "Episode 6560: Finished training\n",
      "Episode 6580: Finished training\n",
      "Episode 6600: Finished training\n",
      "Episode 6620: Finished training\n",
      "Episode 6640: Finished training\n",
      "Episode 6660: Finished training\n",
      "Episode 6680: Finished training\n",
      "Episode 6700: Finished training\n",
      "Episode 6720: Finished training\n",
      "Episode 6740: Finished training\n",
      "Episode 6760: Finished training\n",
      "Episode 6780: Finished training\n",
      "Episode 6800: Finished training\n",
      "Episode 6820: Finished training\n",
      "Episode 6840: Finished training\n",
      "Episode 6860: Finished training\n",
      "Episode 6880: Finished training\n",
      "Episode 6900: Finished training\n",
      "Episode 6920: Finished training\n",
      "Episode 6940: Finished training\n",
      "Episode 6960: Finished training\n",
      "Episode 6980: Finished training\n",
      "Episode 7000: Finished training\n",
      "Episode 7020: Finished training\n",
      "Episode 7040: Finished training\n",
      "Episode 7060: Finished training\n",
      "Episode 7080: Finished training\n",
      "Episode 7100: Finished training\n",
      "Episode 7120: Finished training\n",
      "Episode 7140: Finished training\n",
      "Episode 7160: Finished training\n",
      "Episode 7180: Finished training\n",
      "Episode 7200: Finished training\n",
      "Episode 7220: Finished training\n",
      "Episode 7240: Finished training\n",
      "Episode 7260: Finished training\n",
      "Episode 7280: Finished training\n",
      "Episode 7300: Finished training\n",
      "Episode 7320: Finished training\n",
      "Episode 7340: Finished training\n",
      "Episode 7360: Finished training\n",
      "Episode 7380: Finished training\n",
      "Episode 7400: Finished training\n",
      "Episode 7420: Finished training\n",
      "Episode 7440: Finished training\n",
      "Episode 7460: Finished training\n",
      "Episode 7480: Finished training\n",
      "Episode 7500: Finished training\n",
      "Episode 7520: Finished training\n",
      "Episode 7540: Finished training\n",
      "Episode 7560: Finished training\n",
      "Episode 7580: Finished training\n",
      "Episode 7600: Finished training\n",
      "Episode 7620: Finished training\n",
      "Episode 7640: Finished training\n",
      "Episode 7660: Finished training\n",
      "Episode 7680: Finished training\n",
      "Episode 7700: Finished training\n",
      "Episode 7720: Finished training\n",
      "Episode 7740: Finished training\n",
      "Episode 7760: Finished training\n",
      "Episode 7780: Finished training\n",
      "Episode 7800: Finished training\n",
      "Episode 7820: Finished training\n",
      "Episode 7840: Finished training\n",
      "Episode 7860: Finished training\n",
      "Episode 7880: Finished training\n",
      "Episode 7900: Finished training\n",
      "Episode 7920: Finished training\n",
      "Episode 7940: Finished training\n",
      "Episode 7960: Finished training\n",
      "Episode 7980: Finished training\n",
      "Episode 8000: Finished training\n",
      "Episode 8020: Finished training\n",
      "Episode 8040: Finished training\n",
      "Episode 8060: Finished training\n",
      "Episode 8080: Finished training\n",
      "Episode 8100: Finished training\n",
      "Episode 8120: Finished training\n",
      "Episode 8140: Finished training\n",
      "Episode 8160: Finished training\n",
      "Episode 8180: Finished training\n",
      "Episode 8200: Finished training\n",
      "Episode 8220: Finished training\n",
      "Episode 8240: Finished training\n",
      "Episode 8260: Finished training\n",
      "Episode 8280: Finished training\n",
      "Episode 8300: Finished training\n",
      "Episode 8320: Finished training\n",
      "Episode 8340: Finished training\n",
      "Episode 8360: Finished training\n",
      "Episode 8380: Finished training\n",
      "Episode 8400: Finished training\n",
      "Episode 8420: Finished training\n",
      "Episode 8440: Finished training\n",
      "Episode 8460: Finished training\n",
      "Episode 8480: Finished training\n",
      "Episode 8500: Finished training\n",
      "Episode 8520: Finished training\n",
      "Episode 8540: Finished training\n",
      "Episode 8560: Finished training\n",
      "Episode 8580: Finished training\n",
      "Episode 8600: Finished training\n",
      "Episode 8620: Finished training\n",
      "Episode 8640: Finished training\n",
      "Episode 8660: Finished training\n",
      "Episode 8680: Finished training\n",
      "Episode 8700: Finished training\n",
      "Episode 8720: Finished training\n",
      "Episode 8740: Finished training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#env = gym.make(\"CustomCartPole-v2\")\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m episode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_episodes):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# Reset the environment\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      7\u001B[0m     epsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(epsilon_end, epsilon_start \u001B[38;5;241m*\u001B[39m (epsilon_decay_rate \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m episode))\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# Run one episode\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:400\u001B[0m, in \u001B[0;36mOrderEnforcing.reset\u001B[1;34m(self, seed, options)\u001B[0m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001B[39;00m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 400\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\gymnasium\\core.py:328\u001B[0m, in \u001B[0;36mWrapper.reset\u001B[1;34m(self, seed, options)\u001B[0m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m, seed: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, options: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    326\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[0;32m    327\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:295\u001B[0m, in \u001B[0;36mPassiveEnvChecker.reset\u001B[1;34m(self, seed, options)\u001B[0m\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_reset_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, seed\u001B[38;5;241m=\u001B[39mseed, options\u001B[38;5;241m=\u001B[39moptions)\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\cartpole_irl.py:241\u001B[0m, in \u001B[0;36mCustomCartPoleEnv.reset\u001B[1;34m(self, seed, options)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mreset(seed\u001B[38;5;241m=\u001B[39mseed)\n\u001B[0;32m    238\u001B[0m \u001B[38;5;66;03m# Note that if you use custom reset bounds, it may lead to out-of-bound\u001B[39;00m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;66;03m# state/observations.\u001B[39;00m\n\u001B[1;32m--> 241\u001B[0m \u001B[43mmachine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperform_reset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marduino\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprinter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetState(), dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps_over_threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\machine.py:198\u001B[0m, in \u001B[0;36mperform_reset\u001B[1;34m(arduino, printer)\u001B[0m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mperform_reset\u001B[39m(arduino, printer):\n\u001B[0;32m    197\u001B[0m     move_to_position(printer, \u001B[38;5;241m110\u001B[39m, feedrate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3000\u001B[39m)\n\u001B[1;32m--> 198\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m---> 11\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mnew_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     nextStep \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     14\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m nextStep[\u001B[38;5;241m0\u001B[39m]\n",
      "Cell \u001B[0;32mIn[3], line 28\u001B[0m, in \u001B[0;36mDQNAgent.act\u001B[0;34m(self, state, eps)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mact\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, eps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.\u001B[39m):\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;66;03m#print(type(state))\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m     state_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnetwork_local\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the agent's performance\n",
    "test_episodes = 5\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = new_agent.act(state, eps=0.)\n",
    "        \n",
    "        nextStep = env.step(action)\n",
    "        next_state = nextStep[0]\n",
    "        reward = nextStep[1]\n",
    "        done = nextStep[2]\n",
    "        \n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "average_reward = sum(episode_rewards) / test_episodes\n",
    "print(f\"Average reward over {test_episodes} test episodes: {average_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 36\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# Use the agent's action when no key is pressed\u001B[39;00m\n\u001B[1;32m     34\u001B[0m     action \u001B[38;5;241m=\u001B[39m new_agent\u001B[38;5;241m.\u001B[39mact(state, eps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.\u001B[39m)\n\u001B[0;32m---> 36\u001B[0m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m nextStep \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m#nextStep = env.step(2)\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/cartpole/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:409\u001B[0m, in \u001B[0;36mOrderEnforcing.render\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disable_render_order_enforcing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m    405\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\n\u001B[1;32m    406\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    407\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    408\u001B[0m     )\n\u001B[0;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/cartpole/venv/lib/python3.11/site-packages/gymnasium/core.py:332\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RenderFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[RenderFrame] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/cartpole/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:303\u001B[0m, in \u001B[0;36mPassiveEnvChecker.render\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_render_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv)\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/cartpole/custom_cartpole_v2.py:381\u001B[0m, in \u001B[0;36mCustomCartPoleEnv.render\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    380\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[0;32m--> 381\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mflip()\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Visualize the agent's performance\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Define key mappings for arrow keys\n",
    "KEY_MAPPING = {\n",
    "    pygame.K_LEFT: 0,   # Left arrow key (action 0)\n",
    "    pygame.K_RIGHT: 4,  # Right arrow key (action 1)\n",
    "}\n",
    "\n",
    "#close old env\n",
    "env.close()\n",
    "\n",
    "env = gym.make(\"CustomCartPole-v2\", render_mode=\"human\")\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            done = True\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    if keys[pygame.K_LEFT]:\n",
    "        action = KEY_MAPPING[pygame.K_LEFT]  # Left arrow key overrides agent's action\n",
    "    elif keys[pygame.K_RIGHT]:\n",
    "        action = KEY_MAPPING[pygame.K_RIGHT]  # Right arrow key overrides agent's action\n",
    "    else:\n",
    "        # Use the agent's action when no key is pressed\n",
    "        action = new_agent.act(state, eps=0.)\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "\n",
    "    nextStep = env.step(action)\n",
    "    #nextStep = env.step(2)\n",
    "    next_state = nextStep[0]\n",
    "    reward = nextStep[1]\n",
    "    done = nextStep[2]\n",
    "\n",
    "    state = next_state\n",
    "    #time.sleep(0.1)  Add a delay to make the visualization easier to follow\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T00:29:03.251453Z",
     "start_time": "2025-01-04T00:29:02.994387Z"
    }
   },
   "source": "save_model(new_agent, \"agentirlv4.pth\")",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
