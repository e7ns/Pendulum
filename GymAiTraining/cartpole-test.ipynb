{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define the neural network model\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def save_model(agent, filename):\n",
    "    # Save the model's state_dict (weights) for both the local and target Q networks\n",
    "    torch.save({\n",
    "        'state_dict_local': agent.qnetwork_local.state_dict(),\n",
    "        'state_dict_target': agent.qnetwork_target.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "        'seed': agent.seed,\n",
    "        'state_size': agent.state_size,\n",
    "        'action_size': agent.action_size\n",
    "    }, filename)\n",
    "    \n",
    "def load_model(filename, state_size, action_size, seed, lr):\n",
    "    # Initialize a new DQN agent\n",
    "    agent = DQNAgent(state_size, action_size, seed, lr)\n",
    "    \n",
    "    # Load the saved model's state_dicts into the agent\n",
    "    checkpoint = torch.load(filename)\n",
    "    agent.qnetwork_local.load_state_dict(checkpoint['state_dict_local'])\n",
    "    agent.qnetwork_target.load_state_dict(checkpoint['state_dict_target'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Optionally, return additional info if needed\n",
    "    agent.seed = checkpoint['seed']\n",
    "    agent.state_size = checkpoint['state_size']\n",
    "    agent.action_size = checkpoint['action_size']\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN agent class\n",
    "class DQNAgent:\n",
    "    # Initialize the DQN agent\n",
    "    def __init__(self, state_size, action_size, seed, lr):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > 64:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma=0.99)\n",
    "\n",
    "    # Choose an action based on the current state\n",
    "    def act(self, state, eps=0.):\n",
    "        #print(type(state))\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state_tensor)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if np.random.random() > eps:\n",
    "            return action_values.argmax(dim=1).item()\n",
    "        else:\n",
    "            return np.random.randint(self.action_size)\n",
    "\n",
    "    # Learn from batch of experiences\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/qjtm3pr93qb5_xmm6nfh1k040000gp/T/ipykernel_71682/820921630.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and the agent\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "from custom_cartpole_v2 import CustomCartPoleEnv\n",
    "\n",
    "gym.register(\n",
    "    id=\"CustomCartPole-v2\",\n",
    "    entry_point=CustomCartPoleEnv,\n",
    ")\n",
    "# Set up the environment\n",
    "env = gym.make(\"CustomCartPole-v2\")\n",
    "\n",
    "new_agent = load_model(\"agent2kv2fix.pth\", 4, 5, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:27:42.388984Z",
     "start_time": "2024-12-30T18:27:35.424635Z"
    }
   },
   "source": [
    "# Visualize the agent's performance\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Define key mappings for arrow keys\n",
    "KEY_MAPPING = {\n",
    "    pygame.K_LEFT: 0,   # Left arrow key (action 0)\n",
    "    pygame.K_RIGHT: 4,  # Right arrow key (action 1)\n",
    "}\n",
    "\n",
    "#close old env\n",
    "env.close()\n",
    "\n",
    "env = gym.make(\"CustomCartPole-v2\", render_mode=\"human\")\n",
    "\n",
    "while True:\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "    \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                done = true\n",
    "        keys = pygame.key.get_pressed()\n",
    "    \n",
    "        if keys[pygame.K_LEFT]:\n",
    "            action = KEY_MAPPING[pygame.K_LEFT]  # Left arrow key overrides agent's action\n",
    "        elif keys[pygame.K_RIGHT]:\n",
    "            action = KEY_MAPPING[pygame.K_RIGHT]  # Right arrow key overrides agent's action\n",
    "        else:\n",
    "            # Use the agent's action when no key is pressed\n",
    "            action = new_agent.act(state, eps=0.)\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "    \n",
    "        nextStep = env.step(action)\n",
    "        #nextStep = env.step(2)\n",
    "        next_state = nextStep[0]\n",
    "        reward = nextStep[1]\n",
    "        done = nextStep[2]\n",
    "    \n",
    "        state = next_state\n",
    "        #time.sleep(0.1)  Add a delay to make the visualization easier to follow\n",
    "    \n",
    "env.close()    \n",
    "pygame.quit()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 15\u001B[0m\n\u001B[0;32m      9\u001B[0m KEY_MAPPING \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     10\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mK_LEFT: \u001B[38;5;241m0\u001B[39m,   \u001B[38;5;66;03m# Left arrow key (action 0)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mK_RIGHT: \u001B[38;5;241m4\u001B[39m,  \u001B[38;5;66;03m# Right arrow key (action 1)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m }\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m#close old env\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[43menv\u001B[49m\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m     17\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomCartPole-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m, render_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'env' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(new_agent, \"agent2kv2fix.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
