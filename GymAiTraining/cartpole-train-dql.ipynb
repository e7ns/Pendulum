{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T04:25:12.063195Z",
     "start_time": "2025-02-02T04:25:10.641954Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define the neural network model\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T04:25:18.097038Z",
     "start_time": "2025-02-02T04:25:18.091038Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "def save_model(agent, filename):\n",
    "    # Save the model's state_dict (weights) for both the local and target Q networks\n",
    "    torch.save({\n",
    "        'state_dict_local': agent.qnetwork_local.state_dict(),\n",
    "        'state_dict_target': agent.qnetwork_target.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "        'seed': agent.seed,\n",
    "        'state_size': agent.state_size,\n",
    "        'action_size': agent.action_size\n",
    "    }, filename)\n",
    "    \n",
    "def load_model(filename, state_size, action_size, seed, lr):\n",
    "    # Initialize a new DQN agent\n",
    "    agent = DQNAgent(state_size, action_size, seed, lr)\n",
    "    \n",
    "    # Load the saved model's state_dicts into the agent\n",
    "    checkpoint = torch.load(filename)\n",
    "    agent.qnetwork_local.load_state_dict(checkpoint['state_dict_local'])\n",
    "    agent.qnetwork_target.load_state_dict(checkpoint['state_dict_target'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Optionally, return additional info if needed\n",
    "    agent.seed = checkpoint['seed']\n",
    "    agent.state_size = checkpoint['state_size']\n",
    "    agent.action_size = checkpoint['action_size']\n",
    "    \n",
    "    return agent"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T04:25:19.729222Z",
     "start_time": "2025-02-02T04:25:19.722143Z"
    }
   },
   "source": [
    "# Define the DQN agent class\n",
    "class DQNAgent:\n",
    "    # Initialize the DQN agent\n",
    "    def __init__(self, state_size, action_size, seed, lr):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > 64:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma=0.99)\n",
    "\n",
    "    # Choose an action based on the current state\n",
    "    def act(self, state, eps=0.):\n",
    "        #print(type(state))\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state_tensor)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if np.random.random() > eps:\n",
    "            return action_values.argmax(dim=1).item()\n",
    "        else:\n",
    "            return np.random.randint(self.action_size)\n",
    "\n",
    "    # Learn from batch of experiences\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T04:25:23.064756Z",
     "start_time": "2025-02-02T04:25:22.127122Z"
    }
   },
   "source": [
    "# Initialize the environment and the agent\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "from custom_cartpole_v4 import CustomCartPoleEnv\n",
    "\n",
    "gym.register(\n",
    "    id=\"CustomCartPole-v4\",\n",
    "    entry_point=CustomCartPoleEnv,\n",
    ")\n",
    "# Set up the environment\n",
    "env = gym.make(\"CustomCartPole-v4\")\n",
    "\n",
    "# Define training parameters\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.2\n",
    "epsilon_decay_rate = 0.998\n",
    "gamma = 0.9\n",
    "lr = 0.0005\n",
    "buffer_size = 10000\n",
    "buffer = deque(maxlen=buffer_size)\n",
    "batch_size = 128\n",
    "update_frequency = 20\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "print(f\"Input: {input_dim}, Output: {output_dim}\")\n",
    "new_agent = DQNAgent(input_dim, output_dim, seed=170715, lr = lr)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 4, Output: 5\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:33:19.531004Z",
     "start_time": "2025-02-02T04:25:24.579968Z"
    }
   },
   "source": [
    "# Training loop\n",
    "env = gym.make(\"CustomCartPole-v4\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay_rate ** episode))\n",
    "\n",
    "    # Run one episode\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Choose and perform an action\n",
    "        action = new_agent.act(state, epsilon)\n",
    "        nextStep = env.step(action)\n",
    "        next_state = nextStep[0]\n",
    "        reward = nextStep[1]\n",
    "        done = nextStep[2]\n",
    "        \n",
    "        buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(buffer) >= batch_size:\n",
    "            batch = random.sample(buffer, batch_size)\n",
    "            # Update the agent's knowledge\n",
    "            new_agent.learn(batch, gamma)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # Check if the episode has ended\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if (episode + 1) % update_frequency == 0:\n",
    "        print(f\"Episode {episode + 1}: Finished training\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20: Finished training\n",
      "Episode 40: Finished training\n",
      "Episode 60: Finished training\n",
      "Episode 80: Finished training\n",
      "Episode 100: Finished training\n",
      "Episode 120: Finished training\n",
      "Episode 140: Finished training\n",
      "Episode 160: Finished training\n",
      "Episode 180: Finished training\n",
      "Episode 200: Finished training\n",
      "Episode 220: Finished training\n",
      "Episode 240: Finished training\n",
      "Episode 260: Finished training\n",
      "Episode 280: Finished training\n",
      "Episode 300: Finished training\n",
      "Episode 320: Finished training\n",
      "Episode 340: Finished training\n",
      "Episode 360: Finished training\n",
      "Episode 380: Finished training\n",
      "Episode 400: Finished training\n",
      "Episode 420: Finished training\n",
      "Episode 440: Finished training\n",
      "Episode 460: Finished training\n",
      "Episode 480: Finished training\n",
      "Episode 500: Finished training\n",
      "Episode 520: Finished training\n",
      "Episode 540: Finished training\n",
      "Episode 560: Finished training\n",
      "Episode 580: Finished training\n",
      "Episode 600: Finished training\n",
      "Episode 620: Finished training\n",
      "Episode 640: Finished training\n",
      "Episode 660: Finished training\n",
      "Episode 680: Finished training\n",
      "Episode 700: Finished training\n",
      "Episode 720: Finished training\n",
      "Episode 740: Finished training\n",
      "Episode 760: Finished training\n",
      "Episode 780: Finished training\n",
      "Episode 800: Finished training\n",
      "Episode 820: Finished training\n",
      "Episode 840: Finished training\n",
      "Episode 860: Finished training\n",
      "Episode 880: Finished training\n",
      "Episode 900: Finished training\n",
      "Episode 920: Finished training\n",
      "Episode 940: Finished training\n",
      "Episode 960: Finished training\n",
      "Episode 980: Finished training\n",
      "Episode 1000: Finished training\n",
      "Episode 1020: Finished training\n",
      "Episode 1040: Finished training\n",
      "Episode 1060: Finished training\n",
      "Episode 1080: Finished training\n",
      "Episode 1100: Finished training\n",
      "Episode 1120: Finished training\n",
      "Episode 1140: Finished training\n",
      "Episode 1160: Finished training\n",
      "Episode 1180: Finished training\n",
      "Episode 1200: Finished training\n",
      "Episode 1220: Finished training\n",
      "Episode 1240: Finished training\n",
      "Episode 1260: Finished training\n",
      "Episode 1280: Finished training\n",
      "Episode 1300: Finished training\n",
      "Episode 1320: Finished training\n",
      "Episode 1340: Finished training\n",
      "Episode 1360: Finished training\n",
      "Episode 1380: Finished training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 23\u001B[0m\n\u001B[0;32m     21\u001B[0m     batch \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39msample(buffer, batch_size)\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# Update the agent's knowledge\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m     \u001B[43mnew_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Check if the episode has ended\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 52\u001B[0m, in \u001B[0;36mDQNAgent.learn\u001B[1;34m(self, experiences, gamma)\u001B[0m\n\u001B[0;32m     49\u001B[0m Q_targets_next \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnetwork_target(next_states)\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mmax(\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     50\u001B[0m Q_targets \u001B[38;5;241m=\u001B[39m rewards \u001B[38;5;241m+\u001B[39m (gamma \u001B[38;5;241m*\u001B[39m Q_targets_next \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m dones))\n\u001B[1;32m---> 52\u001B[0m Q_expected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mqnetwork_local\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m1\u001B[39m, actions)\n\u001B[0;32m     54\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmse_loss(Q_expected, Q_targets)\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[1], line 27\u001B[0m, in \u001B[0;36mQNetwork.forward\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     25\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(state))\n\u001B[0;32m     26\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x))\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m---> 11\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mnew_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     nextStep \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     14\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m nextStep[\u001B[38;5;241m0\u001B[39m]\n",
      "Cell \u001B[0;32mIn[3], line 28\u001B[0m, in \u001B[0;36mDQNAgent.act\u001B[0;34m(self, state, eps)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mact\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, eps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.\u001B[39m):\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;66;03m#print(type(state))\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m     state_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnetwork_local\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate the agent's performance\n",
    "test_episodes = 5\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = new_agent.act(state, eps=0.)\n",
    "        \n",
    "        nextStep = env.step(action)\n",
    "        next_state = nextStep[0]\n",
    "        reward = nextStep[1]\n",
    "        done = nextStep[2]\n",
    "        \n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "average_reward = sum(episode_rewards) / test_episodes\n",
    "print(f\"Average reward over {test_episodes} test episodes: {average_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:33:25.162912Z",
     "start_time": "2025-02-02T04:33:24.071469Z"
    }
   },
   "source": [
    "# Visualize the agent's performance\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Define key mappings for arrow keys\n",
    "KEY_MAPPING = {\n",
    "    pygame.K_LEFT: 0,   # Left arrow key (action 0)\n",
    "    pygame.K_RIGHT: 4,  # Right arrow key (action 1)\n",
    "}\n",
    "\n",
    "#close old env\n",
    "env.close()\n",
    "\n",
    "env = gym.make(\"CustomCartPole-v4\", render_mode=\"human\")\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            done = True\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    if keys[pygame.K_LEFT]:\n",
    "        action = KEY_MAPPING[pygame.K_LEFT]  # Left arrow key overrides agent's action\n",
    "    elif keys[pygame.K_RIGHT]:\n",
    "        action = KEY_MAPPING[pygame.K_RIGHT]  # Right arrow key overrides agent's action\n",
    "    else:\n",
    "        # Use the agent's action when no key is pressed\n",
    "        action = new_agent.act(state, eps=0.)\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "\n",
    "    nextStep = env.step(action)\n",
    "    #nextStep = env.step(2)\n",
    "    next_state = nextStep[0]\n",
    "    reward = nextStep[1]\n",
    "    done = nextStep[2]\n",
    "\n",
    "    state = next_state\n",
    "    #time.sleep(0.1)  Add a delay to make the visualization easier to follow\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomCartPoleEnv' object has no attribute 'length'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 36\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;66;03m# Use the agent's action when no key is pressed\u001B[39;00m\n\u001B[0;32m     34\u001B[0m     action \u001B[38;5;241m=\u001B[39m new_agent\u001B[38;5;241m.\u001B[39mact(state, eps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.\u001B[39m)\n\u001B[1;32m---> 36\u001B[0m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m nextStep \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m#nextStep = env.step(2)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:409\u001B[0m, in \u001B[0;36mOrderEnforcing.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disable_render_order_enforcing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m    405\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    408\u001B[0m     )\n\u001B[1;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\gymnasium\\core.py:332\u001B[0m, in \u001B[0;36mWrapper.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RenderFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[RenderFrame] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 332\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:301\u001B[0m, in \u001B[0;36mPassiveEnvChecker.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_render \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    300\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_render \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menv_render_passive_checker\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mrender()\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:361\u001B[0m, in \u001B[0;36menv_render_passive_checker\u001B[1;34m(env)\u001B[0m\n\u001B[0;32m    355\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    356\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m env\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m env\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;129;01min\u001B[39;00m render_modes, (\n\u001B[0;32m    357\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    358\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRender mode: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv\u001B[38;5;241m.\u001B[39mrender_mode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, modes: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrender_modes\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    359\u001B[0m         )\n\u001B[1;32m--> 361\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m env\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    363\u001B[0m     _check_render_return(env\u001B[38;5;241m.\u001B[39mrender_mode, result)\n",
      "File \u001B[1;32m~\\PycharmProjects\\cartpole\\custom_cartpole_v4.py:191\u001B[0m, in \u001B[0;36mCustomCartPoleEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    188\u001B[0m polewidth \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10.0\u001B[39m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;66;03m# To shorten the stick, you might choose a smaller multiplier.\u001B[39;00m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# (In v4 originally, polelen was computed as scale * (2 * self.length); here we choose a reduced length.)\u001B[39;00m\n\u001B[1;32m--> 191\u001B[0m polelen \u001B[38;5;241m=\u001B[39m scale \u001B[38;5;241m*\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlength\u001B[49m  \u001B[38;5;66;03m# MODIFIED: using a smaller length multiplier\u001B[39;00m\n\u001B[0;32m    192\u001B[0m cartwidth \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50.0\u001B[39m\n\u001B[0;32m    193\u001B[0m cartheight \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m30.0\u001B[39m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CustomCartPoleEnv' object has no attribute 'length'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T04:24:59.836219Z",
     "start_time": "2025-02-02T04:24:59.827717Z"
    }
   },
   "source": "save_model(new_agent, \"agent3k.pth\")",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43msave_model\u001B[49m(new_agent, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124magent3k.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'save_model' is not defined"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
